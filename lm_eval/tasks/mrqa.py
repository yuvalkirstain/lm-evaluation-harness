import abc
import os
import json
import re
import string
from collections import Counter
from random import shuffle
from string import ascii_uppercase
from lm_eval.base import Task, rf
from ..metrics import mean
from ..utils import sh


def normalize_answer(s):
    """Lower text and remove punctuation, articles and extra whitespace."""

    def remove_articles(text):
        return re.sub(r'\b(a|an|the)\b', ' ', text)

    def white_space_fix(text):
        return ' '.join(text.split())

    def remove_punc(text):
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)

    def lower(text):
        return text.lower()

    return white_space_fix(remove_articles(remove_punc(lower(s))))


def f1_score(prediction, ground_truth):
    prediction_tokens = normalize_answer(prediction).split()
    ground_truth_tokens = normalize_answer(ground_truth).split()
    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0:
        return 0
    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1


def exact_match_score(prediction, ground_truth):
    return (normalize_answer(prediction) == normalize_answer(ground_truth))


def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):
    scores_for_ground_truths = []
    for ground_truth in ground_truths:
        score = metric_fn(prediction, ground_truth)
        scores_for_ground_truths.append(score)
    return max(scores_for_ground_truths)


class MRQA(Task, abc.ABC):
    def __init__(self):
        self.data_dir = f'data/mrqa/{self.dataset_name}'
        self.train_suffix = f"{self.dataset_name}_train"
        self.dev_suffix = f"{self.dataset_name}_dev"
        super().__init__()

    @abc.abstractmethod
    def download(self):
        pass

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return False

    @staticmethod
    def read_jsonl_with_header(path):
        with open(path) as f:
            f.readline()
            split = list(map(json.loads, f))
        return split

    def training_docs(self):
        path = f'{self.data_dir}/{self.train_suffix}.jsonl'
        return self.read_jsonl_with_header(path)

    def validation_docs(self):
        path = f'{self.data_dir}/{self.dev_suffix}.jsonl'
        return self.read_jsonl_with_header(path)

    def fewshot_description(self):
        # TODO: figure out description
        return ""

    def doc_to_text(self, doc):
        assert len(doc["qas"]) == 1
        qa = doc['qas'][0]
        return 'Background: ' + doc['context'] + '\n\n' + 'Question: ' + qa['question'] + '\n\n' + 'Answer:'

    def doc_to_target(self, doc):
        assert len(doc["qas"]) == 1
        qa = doc['qas'][0]
        answer_list = qa["answers"]
        if len(answer_list) > 0:
            answer = answer_list[0]
        else:
            raise ValueError
        return " " + answer.strip()

    def construct_requests(self, doc, ctx):
        """ Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        continuation = rf.greedy_until(ctx, ['\n'])
        return continuation,

    def process_results(self, doc, results):
        """Take a single document and the LM results_old and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results_old of the requests created in construct_requests.
        """
        continuation, = results

        return {
            'exact': metric_max_over_ground_truths(exact_match_score, continuation, doc['qas'][0]['answers']) * 100.0,  # Exact match (the normalized answer exactly match the gold answer)
            'f1': metric_max_over_ground_truths(f1_score, continuation, doc['qas'][0]['answers']) * 100.0,  # The F-score of predicted tokens versus the gold answer
        }

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """

        return {
            'exact': mean,
            # Exact match (the normalized answer exactly match the gold answer)
            'f1': mean,
            # The F-score of predicted tokens versus the gold answer
        }

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {
            'exact': True,  # Exact match (the normalized answer exactly match the gold answer)
            'f1': True,  # The F-score of predicted tokens versus the gold answer
        }


class MRQATriviaQA(MRQA):
    def __init__(self):
        self.dataset_name = 'triviaqa'
        super().__init__()

    def download(self):
        if not os.path.exists(self.data_dir):
            for subset in ['train', 'dev']:
                sh(f"""
mkdir -p {self.data_dir}
wget https://s3.us-east-2.amazonaws.com/mrqa/release/v2/{subset}/TriviaQA-web.jsonl.gz -O {self.data_dir}/{self.dataset_name}_{subset}.jsonl.gz
gunzip {self.data_dir}/{self.dataset_name}_{subset}.jsonl.gz
""")


class MRQANaturalQuestions(MRQA):
    def __init__(self):
        self.dataset_name = 'natural_questions'
        super().__init__()

    def download(self):
        if not os.path.exists(self.data_dir):
            for subset in ['train', 'dev']:
                sh(f"""
    mkdir -p {self.data_dir}
    wget https://s3.us-east-2.amazonaws.com/mrqa/release/v2/{subset}/NaturalQuestionsShort.jsonl.gz -O {self.data_dir}/{self.dataset_name}_{subset}.jsonl.gz
    gunzip {self.data_dir}/{self.dataset_name}_{subset}.jsonl.gz
    """)


class MRQANaturalQuestionsV3(MRQA):
    def __init__(self):
        self.dataset_name = 'nq_v3'
        super().__init__()

    def download(self):
        if not os.path.exists(self.data_dir):
            for subset in ['train', 'dev']:
                sh(f"""
    mkdir -p {self.data_dir}
    wget https://dl.fbaipublicfiles.com/when_do_billions/nq_multiple_choice/natural_questions_{subset}.multiple_choice.v3.jsonl.gz -O {self.data_dir}/{self.dataset_name}_{subset}.jsonl.gz
    gunzip -d {self.data_dir}/{self.dataset_name}_{subset}.jsonl.gz
    """)


class MRQANaturalQuestionsV3Open(MRQANaturalQuestionsV3):
    def doc_to_text(self, doc):
        assert len(doc["qas"]) == 1
        qa = doc['qas'][0]
        return 'Question: ' + qa['question'] + '\n\n' + 'Answer:'


class MRQANaturalQuestionsV3MC(MRQANaturalQuestionsV3):
    def doc_to_text(self, doc):
        assert len(doc["qas"]) == 1
        qa = doc['qas'][0]
        candidates = list({candidate for candidate, candidate_type in qa["candidates"]})
        shuffle(candidates)
        text = f"Question: {qa['question']}\n\n"
        for char, candidate in zip(ascii_uppercase, candidates):
            text += f"{char}: {candidate}\n"
        text += '\nAnswer:'
        return text


class MRQATriviaQAOpen(MRQATriviaQA):
    def doc_to_text(self, doc):
        assert len(doc["qas"]) == 1
        qa = doc['qas'][0]
        return 'Question: ' + qa['question'] + '\n\n' + 'Answer:'


class MRQANaturalQuestionsOpen(MRQANaturalQuestions):
    def doc_to_text(self, doc):
        assert len(doc["qas"]) == 1
        qa = doc['qas'][0]
        return 'Question: ' + qa['question'] + '\n\n' + 'Answer:'